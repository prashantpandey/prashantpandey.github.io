<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Prashant Pandey</title>
    <link>/project/</link>
      <atom:link href="/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 17 Nov 2023 19:31:15 -0700</lastBuildDate>
    <image>
      <url>/media/icon_hu7877d6dae53df7039c44f2ff773100ab_7411_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>/project/</link>
    </image>
    
    <item>
      <title>GPUs</title>
      <link>/project/gpu/</link>
      <pubDate>Fri, 17 Nov 2023 19:31:15 -0700</pubDate>
      <guid>/project/gpu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>In Memory Indexes</title>
      <link>/project/inmemoryindexes/</link>
      <pubDate>Fri, 30 Jun 2023 10:23:43 -0600</pubDate>
      <guid>/project/inmemoryindexes/</guid>
      <description>&lt;p&gt;Modern in-memory indexes such as hash tables and B-trees have been critical for building high-performance data systems over the past few decades. However, given the rapid rise in data sizes and changing hardware we need to rethink the designs of these traditional data structures to keep up with the challenges.
For example, hash tables strive to minimize space while maximizing speed. The most important factor in speed is the number of cache lines accessed during updates and queries. This is especially important on PMEM, which is slower than DRAM and in which writes are more expensive than reads.
Similarly, B-trees are the go-to data structure for in-memory indexes in storage systems and databases. B-trees support both point operations (i.e., inserts and finds) and range operations (i.e., iterators and maps). However, there is an inherent tradeoff between point and range operations since the optimal node size for point operations is much smaller than the optimal node size for range operations. Existing implementations use a relatively small node size in all nodes of the tree to achieve fast point operations at the cost of range operation throughput.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Streaming</title>
      <link>/project/streaming/</link>
      <pubDate>Fri, 03 Jun 2022 13:18:37 -0700</pubDate>
      <guid>/project/streaming/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Terrace</title>
      <link>/project/graphs/</link>
      <pubDate>Wed, 01 Jun 2022 20:54:24 -0700</pubDate>
      <guid>/project/graphs/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Indexing whole SRA</title>
      <link>/project/comp-bio/</link>
      <pubDate>Wed, 01 Jun 2022 20:49:06 -0700</pubDate>
      <guid>/project/comp-bio/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Filters</title>
      <link>/project/filter/</link>
      <pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate>
      <guid>/project/filter/</guid>
      <description>&lt;p&gt;High performance data analytics problems often use
filters to approximately store or count a set of items while trading
off accuracy for space-efficiency. Filters can also address the limited
memory available on accelerators, such as GPUs. We present GQF, a
feature-rich, GPU-optimized counting quotient filter, which provides
both individual and bulk insertions, with different locking strategies
to maximize performance and ensure correctness for each usage
scenario. GQF has all the benefits of a standard counting quotient
filter: good cache locality, high speed, and support for counting,
deletions, and associating small values with items&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
